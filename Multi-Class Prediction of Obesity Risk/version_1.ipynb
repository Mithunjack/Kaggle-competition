{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68479,"databundleVersionId":7609535,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install cmaes","metadata":{"execution":{"iopub.status.busy":"2024-02-08T00:46:33.153232Z","iopub.execute_input":"2024-02-08T00:46:33.15361Z","iopub.status.idle":"2024-02-08T00:46:45.892925Z","shell.execute_reply.started":"2024-02-08T00:46:33.153581Z","shell.execute_reply":"2024-02-08T00:46:45.891777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\n\n# Correct target variable name\ntarget = 'NObeyesdad'\n\n# Splitting the dataset into features and target variable\nX = df.drop(target, axis=1)\ny = df[target]\n\n# Convert categorical features using one-hot encoding\nX = pd.get_dummies(X)\n\n# Splitting the dataset into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the label encoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the target variable\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.transform(y_val)\n\n# Define the objective function to optimize\ndef objective(trial):\n    # Define hyperparameters to search\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 1.0),\n        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n        'reg_lambda': trial.suggest_uniform('reg_lambda', 0.0, 1.0)\n    }\n\n    # Initialize XGBoost classifier\n    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='mlogloss')\n\n    # Fit the model\n    model.fit(X_train, y_train_encoded)\n\n    # Predict on the validation set\n    y_pred_encoded = model.predict(X_val)\n\n    # Decode the predictions back to original labels\n    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(y_val, y_pred)\n\n    return accuracy\n\n# Define study\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.CmaEsSampler())\n\n# Optimize hyperparameters\nstudy.optimize(objective, n_trials=100)\n\n# Get the best hyperparameters\nbest_params = study.best_params\nbest_params['eval_metric'] = 'mlogloss'\n\n# Train the final model with the best hyperparameters\nfinal_model = xgb.XGBClassifier(**best_params, use_label_encoder=False)\nfinal_model.fit(X_train, y_train_encoded)\n\n# Predict on the test set\ny_pred_encoded = final_model.predict(X_val)\n\n# Decode the predictions back to original labels\ny_pred = label_encoder.inverse_transform(y_pred_encoded)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_val, y_pred)\nprint('Validation Accuracy:', accuracy)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T00:47:26.732681Z","iopub.execute_input":"2024-02-08T00:47:26.733106Z","iopub.status.idle":"2024-02-08T01:04:12.60594Z","shell.execute_reply.started":"2024-02-08T00:47:26.733049Z","shell.execute_reply":"2024-02-08T01:04:12.604593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\n\n# Preprocess the test dataset (e.g., one-hot encoding for categorical features)\ntest_X = pd.get_dummies(test_df)\n\n# Reorder columns in the test dataset to match the order of columns in the training dataset\ntest_X = test_X.reindex(columns=X.columns, fill_value=0)\n\n# Predict on the test set using the final model\ntest_y_pred_encoded = final_model.predict(test_X)\n\n# Decode the predictions back to original labels\ntest_y_pred = label_encoder.inverse_transform(test_y_pred_encoded)\n\n# Create a DataFrame for submission\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'NObeyesdad': test_y_pred\n})\n\n# Save the submission DataFrame to a CSV file\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T01:06:02.582018Z","iopub.execute_input":"2024-02-08T01:06:02.582547Z","iopub.status.idle":"2024-02-08T01:06:03.066374Z","shell.execute_reply.started":"2024-02-08T01:06:02.582513Z","shell.execute_reply":"2024-02-08T01:06:03.065377Z"},"trusted":true},"execution_count":null,"outputs":[]}]}